{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**bold**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "printmd('**bold**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/train.csv\"\n",
    "data_raw = pd.read_csv(data_path)\n",
    "#data_raw = data_raw.loc[np.random.choice(data_raw.index, size=2000)]\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "categories = list(data_raw.columns.values)\n",
    "categories = categories[2:]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'obscene']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"toxic&obscene\"\n",
    "tasks = query.split('&')\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic&obscene']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.split('^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Repository\n",
    "### From JSON to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model/OvR_identity_hate.json', 'model/BinaryRelevance.json', 'model/MLkNN.json', 'model/OvR_threat.json', 'model/OvR_obscene.json', 'model/OvR_severe_toxic.json', 'model/ClassifierChain.json', 'model/OvR_insult.json', 'model/LabelPowerset.json', 'model/OvR_toxic.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model', 'OvR_identity_hate', 'json']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "files = glob.glob(\"model/*.json\")\n",
    "print(files)\n",
    "re.split('/|\\\\.', files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/OvR_identity_hate.json\n",
      "['identity_hate']\n",
      "model/BinaryRelevance.json\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "model/MLkNN.json\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "model/OvR_threat.json\n",
      "['threat']\n",
      "model/OvR_obscene.json\n",
      "['obscene']\n",
      "model/OvR_severe_toxic.json\n",
      "['severe_toxic']\n",
      "model/ClassifierChain.json\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "model/OvR_insult.json\n",
      "['insult']\n",
      "model/LabelPowerset.json\n",
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "model/OvR_toxic.json\n",
      "['toxic']\n"
     ]
    }
   ],
   "source": [
    "model_repository_accuracy = {}\n",
    "model_repository_cost = {}\n",
    "for file in files:\n",
    "    print(file)\n",
    "    model = re.split('/|\\\\.', file)[1]\n",
    "#     print(model)\n",
    "    with open(file) as json_file: \n",
    "        data = json.load(json_file)\n",
    "        print(list(data['task'].keys()))\n",
    "        model_repository_accuracy[model] = {}\n",
    "        model_repository_cost[model] = {}\n",
    "        for key in list(data['task'].keys()):\n",
    "            model_repository_accuracy[model][key] = data['task'][key]['accuracy']\n",
    "            model_repository_cost[model][key] = round(data['task'][key]['cost'],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   identity_hate  toxic  severe_toxic  obscene  threat  insult\n",
      "OvR_identity_hate          0.995  0.000         0.000    0.000   0.000   0.000\n",
      "BinaryRelevance            0.990  0.887         0.978    0.937   0.990   0.932\n",
      "MLkNN                      0.995  0.910         0.992    0.955   0.992   0.957\n",
      "ClassifierChain            0.995  0.898         0.988    0.950   0.992   0.942\n",
      "LabelPowerset              0.995  0.898         0.988    0.950   0.992   0.942\n",
      "OvR_toxic                  0.000  0.898         0.000    0.000   0.000   0.000\n",
      "OvR_severe_toxic           0.000  0.000         0.988    0.000   0.000   0.000\n",
      "OvR_obscene                0.000  0.000         0.000    0.950   0.000   0.000\n",
      "OvR_threat                 0.000  0.000         0.000    0.000   0.992   0.000\n",
      "OvR_insult                 0.000  0.000         0.000    0.000   0.000   0.942\n",
      "                   identity_hate      toxic  severe_toxic    obscene  \\\n",
      "OvR_identity_hate        0.00040  200.00000     200.00000  200.00000   \n",
      "BinaryRelevance          3.45655    3.45655       3.45655    3.45655   \n",
      "MLkNN                   51.26517   51.26517      51.26517   51.26517   \n",
      "ClassifierChain          0.62730    0.62730       0.62730    0.62730   \n",
      "LabelPowerset            0.13773    0.13773       0.13773    0.13773   \n",
      "OvR_toxic              200.00000    0.00037     200.00000  200.00000   \n",
      "OvR_severe_toxic       200.00000  200.00000       0.00036  200.00000   \n",
      "OvR_obscene            200.00000  200.00000     200.00000    0.00036   \n",
      "OvR_threat             200.00000  200.00000     200.00000  200.00000   \n",
      "OvR_insult             200.00000  200.00000     200.00000  200.00000   \n",
      "\n",
      "                      threat     insult  \n",
      "OvR_identity_hate  200.00000  200.00000  \n",
      "BinaryRelevance      3.45655    3.45655  \n",
      "MLkNN               51.26517   51.26517  \n",
      "ClassifierChain      0.62730    0.62730  \n",
      "LabelPowerset        0.13773    0.13773  \n",
      "OvR_toxic          200.00000  200.00000  \n",
      "OvR_severe_toxic   200.00000  200.00000  \n",
      "OvR_obscene        200.00000  200.00000  \n",
      "OvR_threat           0.00047  200.00000  \n",
      "OvR_insult         200.00000    0.00048  \n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "model_repository_acc_df = DataFrame.from_dict(model_repository_accuracy,orient='index')\n",
    "model_repository_acc_df = model_repository_acc_df.fillna(0.0)\n",
    "print(model_repository_acc_df)\n",
    "\n",
    "model_repository_cost_df = DataFrame.from_dict(model_repository_cost,orient='index')\n",
    "model_repository_cost_df = model_repository_cost_df.fillna(200)\n",
    "print(model_repository_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   identity_hate      toxic  severe_toxic    obscene  \\\n",
      "OvR_identity_hate       0.000397        NaN           NaN        NaN   \n",
      "BinaryRelevance         3.456550   3.456550      3.456550   3.456550   \n",
      "MLkNN                  51.265170  51.265170     51.265170  51.265170   \n",
      "ClassifierChain         0.627300   0.627300      0.627300   0.627300   \n",
      "LabelPowerset           0.137730   0.137730      0.137730   0.137730   \n",
      "OvR_toxic                    NaN   0.000367           NaN        NaN   \n",
      "OvR_severe_toxic             NaN        NaN      0.000361        NaN   \n",
      "OvR_obscene                  NaN        NaN           NaN   0.000355   \n",
      "OvR_threat                   NaN        NaN           NaN        NaN   \n",
      "OvR_insult                   NaN        NaN           NaN        NaN   \n",
      "\n",
      "                      threat     insult  \n",
      "OvR_identity_hate        NaN        NaN  \n",
      "BinaryRelevance     3.456550   3.456550  \n",
      "MLkNN              51.265170  51.265170  \n",
      "ClassifierChain     0.627300   0.627300  \n",
      "LabelPowerset       0.137730   0.137730  \n",
      "OvR_toxic                NaN        NaN  \n",
      "OvR_severe_toxic         NaN        NaN  \n",
      "OvR_obscene              NaN        NaN  \n",
      "OvR_threat          0.000467        NaN  \n",
      "OvR_insult               NaN   0.000476  \n"
     ]
    }
   ],
   "source": [
    "model_repository_cost_df.fillna(200)\n",
    "print(model_repository_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OvR_identity_hate  identity_hate    0.995\n",
       "BinaryRelevance    identity_hate    0.990\n",
       "                   toxic            0.887\n",
       "                   severe_toxic     0.978\n",
       "                   obscene          0.937\n",
       "                   threat           0.990\n",
       "                   insult           0.932\n",
       "MLkNN              identity_hate    0.995\n",
       "                   toxic            0.910\n",
       "                   severe_toxic     0.992\n",
       "                   obscene          0.955\n",
       "                   threat           0.992\n",
       "                   insult           0.957\n",
       "ClassifierChain    identity_hate    0.995\n",
       "                   toxic            0.898\n",
       "                   severe_toxic     0.988\n",
       "                   obscene          0.950\n",
       "                   threat           0.992\n",
       "                   insult           0.942\n",
       "LabelPowerset      identity_hate    0.995\n",
       "                   toxic            0.898\n",
       "                   severe_toxic     0.988\n",
       "                   obscene          0.950\n",
       "                   threat           0.992\n",
       "                   insult           0.942\n",
       "OvR_toxic          toxic            0.898\n",
       "OvR_severe_toxic   severe_toxic     0.988\n",
       "OvR_obscene        obscene          0.950\n",
       "OvR_threat         threat           0.992\n",
       "OvR_insult         insult           0.942\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repository_acc_df.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From DataFrame to Gurobi multidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['identity_hate', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repository_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['OvR_identity_hate', 'BinaryRelevance', 'MLkNN', 'ClassifierChain',\n",
       "       'LabelPowerset', 'OvR_toxic', 'OvR_severe_toxic', 'OvR_obscene',\n",
       "       'OvR_threat', 'OvR_insult'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_repository_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('OvR_identity_hate', 'identity_hate'): 0.995,\n",
       " ('BinaryRelevance', 'identity_hate'): 0.99,\n",
       " ('MLkNN', 'identity_hate'): 0.995,\n",
       " ('ClassifierChain', 'identity_hate'): 0.995,\n",
       " ('LabelPowerset', 'identity_hate'): 0.995,\n",
       " ('OvR_toxic', 'identity_hate'): 0.0,\n",
       " ('OvR_severe_toxic', 'identity_hate'): 0.0,\n",
       " ('OvR_obscene', 'identity_hate'): 0.0,\n",
       " ('OvR_threat', 'identity_hate'): 0.0,\n",
       " ('OvR_insult', 'identity_hate'): 0.0,\n",
       " ('OvR_identity_hate', 'toxic'): 0.0,\n",
       " ('BinaryRelevance', 'toxic'): 0.887,\n",
       " ('MLkNN', 'toxic'): 0.91,\n",
       " ('ClassifierChain', 'toxic'): 0.898,\n",
       " ('LabelPowerset', 'toxic'): 0.898,\n",
       " ('OvR_toxic', 'toxic'): 0.898,\n",
       " ('OvR_severe_toxic', 'toxic'): 0.0,\n",
       " ('OvR_obscene', 'toxic'): 0.0,\n",
       " ('OvR_threat', 'toxic'): 0.0,\n",
       " ('OvR_insult', 'toxic'): 0.0,\n",
       " ('OvR_identity_hate', 'severe_toxic'): 0.0,\n",
       " ('BinaryRelevance', 'severe_toxic'): 0.978,\n",
       " ('MLkNN', 'severe_toxic'): 0.992,\n",
       " ('ClassifierChain', 'severe_toxic'): 0.988,\n",
       " ('LabelPowerset', 'severe_toxic'): 0.988,\n",
       " ('OvR_toxic', 'severe_toxic'): 0.0,\n",
       " ('OvR_severe_toxic', 'severe_toxic'): 0.988,\n",
       " ('OvR_obscene', 'severe_toxic'): 0.0,\n",
       " ('OvR_threat', 'severe_toxic'): 0.0,\n",
       " ('OvR_insult', 'severe_toxic'): 0.0,\n",
       " ('OvR_identity_hate', 'obscene'): 0.0,\n",
       " ('BinaryRelevance', 'obscene'): 0.937,\n",
       " ('MLkNN', 'obscene'): 0.955,\n",
       " ('ClassifierChain', 'obscene'): 0.95,\n",
       " ('LabelPowerset', 'obscene'): 0.95,\n",
       " ('OvR_toxic', 'obscene'): 0.0,\n",
       " ('OvR_severe_toxic', 'obscene'): 0.0,\n",
       " ('OvR_obscene', 'obscene'): 0.95,\n",
       " ('OvR_threat', 'obscene'): 0.0,\n",
       " ('OvR_insult', 'obscene'): 0.0,\n",
       " ('OvR_identity_hate', 'threat'): 0.0,\n",
       " ('BinaryRelevance', 'threat'): 0.99,\n",
       " ('MLkNN', 'threat'): 0.992,\n",
       " ('ClassifierChain', 'threat'): 0.992,\n",
       " ('LabelPowerset', 'threat'): 0.992,\n",
       " ('OvR_toxic', 'threat'): 0.0,\n",
       " ('OvR_severe_toxic', 'threat'): 0.0,\n",
       " ('OvR_obscene', 'threat'): 0.0,\n",
       " ('OvR_threat', 'threat'): 0.992,\n",
       " ('OvR_insult', 'threat'): 0.0,\n",
       " ('OvR_identity_hate', 'insult'): 0.0,\n",
       " ('BinaryRelevance', 'insult'): 0.932,\n",
       " ('MLkNN', 'insult'): 0.957,\n",
       " ('ClassifierChain', 'insult'): 0.942,\n",
       " ('LabelPowerset', 'insult'): 0.942,\n",
       " ('OvR_toxic', 'insult'): 0.0,\n",
       " ('OvR_severe_toxic', 'insult'): 0.0,\n",
       " ('OvR_obscene', 'insult'): 0.0,\n",
       " ('OvR_threat', 'insult'): 0.0,\n",
       " ('OvR_insult', 'insult'): 0.942}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_acc_dict = {(idx,col): model_repository_acc_df[col][idx] for col in model_repository_acc_df.columns for idx in model_repository_df.index}\n",
    "repository_acc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From JSON to multidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.multi.MultiIndex'>\n",
      "[('OvR_identity_hate', 'identity_hate'), ('OvR_identity_hate', 'toxic'), ('OvR_identity_hate', 'severe_toxic'), ('OvR_identity_hate', 'obscene'), ('OvR_identity_hate', 'threat'), ('OvR_identity_hate', 'insult'), ('BinaryRelevance', 'identity_hate'), ('BinaryRelevance', 'toxic'), ('BinaryRelevance', 'severe_toxic'), ('BinaryRelevance', 'obscene'), ('BinaryRelevance', 'threat'), ('BinaryRelevance', 'insult'), ('MLkNN', 'identity_hate'), ('MLkNN', 'toxic'), ('MLkNN', 'severe_toxic'), ('MLkNN', 'obscene'), ('MLkNN', 'threat'), ('MLkNN', 'insult'), ('ClassifierChain', 'identity_hate'), ('ClassifierChain', 'toxic'), ('ClassifierChain', 'severe_toxic'), ('ClassifierChain', 'obscene'), ('ClassifierChain', 'threat'), ('ClassifierChain', 'insult'), ('LabelPowerset', 'identity_hate'), ('LabelPowerset', 'toxic'), ('LabelPowerset', 'severe_toxic'), ('LabelPowerset', 'obscene'), ('LabelPowerset', 'threat'), ('LabelPowerset', 'insult'), ('OvR_toxic', 'identity_hate'), ('OvR_toxic', 'toxic'), ('OvR_toxic', 'severe_toxic'), ('OvR_toxic', 'obscene'), ('OvR_toxic', 'threat'), ('OvR_toxic', 'insult'), ('OvR_severe_toxic', 'identity_hate'), ('OvR_severe_toxic', 'toxic'), ('OvR_severe_toxic', 'severe_toxic'), ('OvR_severe_toxic', 'obscene'), ('OvR_severe_toxic', 'threat'), ('OvR_severe_toxic', 'insult'), ('OvR_obscene', 'identity_hate'), ('OvR_obscene', 'toxic'), ('OvR_obscene', 'severe_toxic'), ('OvR_obscene', 'obscene'), ('OvR_obscene', 'threat'), ('OvR_obscene', 'insult'), ('OvR_threat', 'identity_hate'), ('OvR_threat', 'toxic'), ('OvR_threat', 'severe_toxic'), ('OvR_threat', 'obscene'), ('OvR_threat', 'threat'), ('OvR_threat', 'insult'), ('OvR_insult', 'identity_hate'), ('OvR_insult', 'toxic'), ('OvR_insult', 'severe_toxic'), ('OvR_insult', 'obscene'), ('OvR_insult', 'threat'), ('OvR_insult', 'insult')]\n"
     ]
    }
   ],
   "source": [
    "multi_index = pd.MultiIndex.from_product([model_repository_df.index, model_repository_df.columns], names=['first', 'second'])\n",
    "print(type(multi_index))\n",
    "list_multi_index = multi_index.tolist()\n",
    "print(list_multi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('OvR_identity_hate', 'identity_hate'): [0.995,\n",
       "  0.00039699999999999995,\n",
       "  -0.005012541823544286],\n",
       " ('BinaryRelevance', 'toxic'): [0.887, 3.45655, -0.11991029667255755],\n",
       " ('BinaryRelevance', 'severe_toxic'): [0.978, 3.45655, -0.022245608947319737],\n",
       " ('BinaryRelevance', 'obscene'): [0.937, 3.45655, -0.0650719967437148],\n",
       " ('BinaryRelevance', 'threat'): [0.99, 3.45655, -0.01005033585350145],\n",
       " ('BinaryRelevance', 'insult'): [0.932, 3.45655, -0.07042246429654582],\n",
       " ('BinaryRelevance', 'identity_hate'): [0.99, 3.45655, -0.01005033585350145],\n",
       " ('MLkNN', 'toxic'): [0.91, 51.26517, -0.09431067947124129],\n",
       " ('MLkNN', 'severe_toxic'): [0.992, 51.26517, -0.008032171697264267],\n",
       " ('MLkNN', 'obscene'): [0.955, 51.26517, -0.046043938501406846],\n",
       " ('MLkNN', 'threat'): [0.992, 51.26517, -0.008032171697264267],\n",
       " ('MLkNN', 'insult'): [0.957, 51.26517, -0.04395188752918283],\n",
       " ('MLkNN', 'identity_hate'): [0.995, 51.26517, -0.005012541823544286],\n",
       " ('OvR_threat', 'threat'): [0.992,\n",
       "  0.00046699999999999997,\n",
       "  -0.008032171697264267],\n",
       " ('OvR_obscene', 'obscene'): [0.95, 0.000355, -0.05129329438755058],\n",
       " ('OvR_severe_toxic', 'severe_toxic'): [0.988,\n",
       "  0.000361,\n",
       "  -0.012072581234269249],\n",
       " ('ClassifierChain', 'toxic'): [0.898, 0.6273, -0.10758521067993743],\n",
       " ('ClassifierChain', 'severe_toxic'): [0.988, 0.6273, -0.012072581234269249],\n",
       " ('ClassifierChain', 'obscene'): [0.95, 0.6273, -0.05129329438755058],\n",
       " ('ClassifierChain', 'threat'): [0.992, 0.6273, -0.008032171697264267],\n",
       " ('ClassifierChain', 'insult'): [0.942, 0.6273, -0.05975000440577405],\n",
       " ('ClassifierChain', 'identity_hate'): [0.995, 0.6273, -0.005012541823544286],\n",
       " ('OvR_insult', 'insult'): [0.942, 0.000476, -0.05975000440577405],\n",
       " ('LabelPowerset', 'toxic'): [0.898, 0.13773, -0.10758521067993743],\n",
       " ('LabelPowerset', 'severe_toxic'): [0.988, 0.13773, -0.012072581234269249],\n",
       " ('LabelPowerset', 'obscene'): [0.95, 0.13773, -0.05129329438755058],\n",
       " ('LabelPowerset', 'threat'): [0.992, 0.13773, -0.008032171697264267],\n",
       " ('LabelPowerset', 'insult'): [0.942, 0.13773, -0.05975000440577405],\n",
       " ('LabelPowerset', 'identity_hate'): [0.995, 0.13773, -0.005012541823544286],\n",
       " ('OvR_toxic', 'toxic'): [0.898, 0.00036700000000000003, -0.10758521067993743],\n",
       " ('OvR_identity_hate', 'toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_identity_hate', 'severe_toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_identity_hate', 'obscene'): [0.0, 200, 0.0005],\n",
       " ('OvR_identity_hate', 'threat'): [0.0, 200, 0.0005],\n",
       " ('OvR_identity_hate', 'insult'): [0.0, 200, 0.0005],\n",
       " ('OvR_toxic', 'identity_hate'): [0.0, 200, 0.0005],\n",
       " ('OvR_toxic', 'severe_toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_toxic', 'obscene'): [0.0, 200, 0.0005],\n",
       " ('OvR_toxic', 'threat'): [0.0, 200, 0.0005],\n",
       " ('OvR_toxic', 'insult'): [0.0, 200, 0.0005],\n",
       " ('OvR_severe_toxic', 'identity_hate'): [0.0, 200, 0.0005],\n",
       " ('OvR_severe_toxic', 'toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_severe_toxic', 'obscene'): [0.0, 200, 0.0005],\n",
       " ('OvR_severe_toxic', 'threat'): [0.0, 200, 0.0005],\n",
       " ('OvR_severe_toxic', 'insult'): [0.0, 200, 0.0005],\n",
       " ('OvR_obscene', 'identity_hate'): [0.0, 200, 0.0005],\n",
       " ('OvR_obscene', 'toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_obscene', 'severe_toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_obscene', 'threat'): [0.0, 200, 0.0005],\n",
       " ('OvR_obscene', 'insult'): [0.0, 200, 0.0005],\n",
       " ('OvR_threat', 'identity_hate'): [0.0, 200, 0.0005],\n",
       " ('OvR_threat', 'toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_threat', 'severe_toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_threat', 'obscene'): [0.0, 200, 0.0005],\n",
       " ('OvR_threat', 'insult'): [0.0, 200, 0.0005],\n",
       " ('OvR_insult', 'identity_hate'): [0.0, 200, 0.0005],\n",
       " ('OvR_insult', 'toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_insult', 'severe_toxic'): [0.0, 200, 0.0005],\n",
       " ('OvR_insult', 'obscene'): [0.0, 200, 0.0005],\n",
       " ('OvR_insult', 'threat'): [0.0, 200, 0.0005]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_repository_acc_from_json = {}\n",
    "dict_repository_cost_from_json = {}\n",
    "dict_repository_from_json = {}\n",
    "list_of_model = []\n",
    "for file in files:\n",
    "    model = re.split('/|\\\\.', file)[1]\n",
    "    list_of_model.append(model)\n",
    "    with open(file) as json_file: \n",
    "        data = json.load(json_file)\n",
    "#         print(list(data['task'].keys()))\n",
    "        model_repository_cost[model] = {}\n",
    "        for key in list(data['task'].keys()):\n",
    "            dict_repository_acc_from_json[(model,key)] = data['task'][key]['accuracy']\n",
    "            dict_repository_cost_from_json[(model,key)] = data['task'][key]['cost']\n",
    "            dict_repository_from_json[(model,key)] = [data['task'][key]['accuracy'], data['task'][key]['cost'], log(data['task'][key]['accuracy'])]\n",
    "            \n",
    "for tuple_key in list_multi_index:\n",
    "    if tuple_key not in dict_repository_from_json.keys():\n",
    "        dict_repository_acc_from_json[tuple_key] = 0.0\n",
    "        dict_repository_cost_from_json[tuple_key] = 200\n",
    "        dict_repository_from_json[tuple_key] = [0.0,200,0.0005]\n",
    "dict_repository_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gurobi Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = list_of_model\n",
    "T = ['toxic','obscene']\n",
    "combinations, accuracy, speed, log_accuracy = gp.multidict(\n",
    "    dict_repository_from_json\n",
    ")\n",
    "dict_model_acc_cost = {model:[model_repository_acc_df[t][model] for t in T]+[model_repository_cost_df[t][model] for t in T] for model in M}\n",
    "Model, acc_task1, acc_task2, cost_task1, cost_task2 = gp.multidict(dict_model_acc_cost)\n",
    "# Objective Constraint\n",
    "Speed = 60\n",
    "Accuracy = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OvR_identity_hate': [0.0, 0.0, 200.0, 200.0],\n",
       " 'BinaryRelevance': [0.887, 0.937, 3.45655, 3.45655],\n",
       " 'MLkNN': [0.91, 0.955, 51.26517, 51.26517],\n",
       " 'OvR_threat': [0.0, 0.0, 200.0, 200.0],\n",
       " 'OvR_obscene': [0.0, 0.95, 200.0, 0.00036],\n",
       " 'OvR_severe_toxic': [0.0, 0.0, 200.0, 200.0],\n",
       " 'ClassifierChain': [0.898, 0.95, 0.6273, 0.6273],\n",
       " 'OvR_insult': [0.0, 0.0, 200.0, 200.0],\n",
       " 'LabelPowerset': [0.898, 0.95, 0.13773, 0.13773],\n",
       " 'OvR_toxic': [0.898, 0.0, 0.00037, 200.0]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_model_acc_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('OvR_identity_hate', 'identity_hate'): 0.995,\n",
       " ('BinaryRelevance', 'toxic'): 0.887,\n",
       " ('BinaryRelevance', 'severe_toxic'): 0.978,\n",
       " ('BinaryRelevance', 'obscene'): 0.937,\n",
       " ('BinaryRelevance', 'threat'): 0.99,\n",
       " ('BinaryRelevance', 'insult'): 0.932,\n",
       " ('BinaryRelevance', 'identity_hate'): 0.99,\n",
       " ('MLkNN', 'toxic'): 0.91,\n",
       " ('MLkNN', 'severe_toxic'): 0.992,\n",
       " ('MLkNN', 'obscene'): 0.955,\n",
       " ('MLkNN', 'threat'): 0.992,\n",
       " ('MLkNN', 'insult'): 0.957,\n",
       " ('MLkNN', 'identity_hate'): 0.995,\n",
       " ('OvR_threat', 'threat'): 0.992,\n",
       " ('OvR_obscene', 'obscene'): 0.95,\n",
       " ('OvR_severe_toxic', 'severe_toxic'): 0.988,\n",
       " ('ClassifierChain', 'toxic'): 0.898,\n",
       " ('ClassifierChain', 'severe_toxic'): 0.988,\n",
       " ('ClassifierChain', 'obscene'): 0.95,\n",
       " ('ClassifierChain', 'threat'): 0.992,\n",
       " ('ClassifierChain', 'insult'): 0.942,\n",
       " ('ClassifierChain', 'identity_hate'): 0.995,\n",
       " ('OvR_insult', 'insult'): 0.942,\n",
       " ('LabelPowerset', 'toxic'): 0.898,\n",
       " ('LabelPowerset', 'severe_toxic'): 0.988,\n",
       " ('LabelPowerset', 'obscene'): 0.95,\n",
       " ('LabelPowerset', 'threat'): 0.992,\n",
       " ('LabelPowerset', 'insult'): 0.942,\n",
       " ('LabelPowerset', 'identity_hate'): 0.995,\n",
       " ('OvR_toxic', 'toxic'): 0.898,\n",
       " ('OvR_identity_hate', 'toxic'): 0.0,\n",
       " ('OvR_identity_hate', 'severe_toxic'): 0.0,\n",
       " ('OvR_identity_hate', 'obscene'): 0.0,\n",
       " ('OvR_identity_hate', 'threat'): 0.0,\n",
       " ('OvR_identity_hate', 'insult'): 0.0,\n",
       " ('OvR_toxic', 'identity_hate'): 0.0,\n",
       " ('OvR_toxic', 'severe_toxic'): 0.0,\n",
       " ('OvR_toxic', 'obscene'): 0.0,\n",
       " ('OvR_toxic', 'threat'): 0.0,\n",
       " ('OvR_toxic', 'insult'): 0.0,\n",
       " ('OvR_severe_toxic', 'identity_hate'): 0.0,\n",
       " ('OvR_severe_toxic', 'toxic'): 0.0,\n",
       " ('OvR_severe_toxic', 'obscene'): 0.0,\n",
       " ('OvR_severe_toxic', 'threat'): 0.0,\n",
       " ('OvR_severe_toxic', 'insult'): 0.0,\n",
       " ('OvR_obscene', 'identity_hate'): 0.0,\n",
       " ('OvR_obscene', 'toxic'): 0.0,\n",
       " ('OvR_obscene', 'severe_toxic'): 0.0,\n",
       " ('OvR_obscene', 'threat'): 0.0,\n",
       " ('OvR_obscene', 'insult'): 0.0,\n",
       " ('OvR_threat', 'identity_hate'): 0.0,\n",
       " ('OvR_threat', 'toxic'): 0.0,\n",
       " ('OvR_threat', 'severe_toxic'): 0.0,\n",
       " ('OvR_threat', 'obscene'): 0.0,\n",
       " ('OvR_threat', 'insult'): 0.0,\n",
       " ('OvR_insult', 'identity_hate'): 0.0,\n",
       " ('OvR_insult', 'toxic'): 0.0,\n",
       " ('OvR_insult', 'severe_toxic'): 0.0,\n",
       " ('OvR_insult', 'obscene'): 0.0,\n",
       " ('OvR_insult', 'threat'): 0.0}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using license file /Users/lizy/gurobi.lic\n",
      "Academic license - for non-commercial use only\n"
     ]
    }
   ],
   "source": [
    "m=gp.Model('RAP_document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision variables\n",
    "v = m.addVars(combinations, vtype=GRB.BINARY, name=\"assign\")\n",
    "\n",
    "v1 = m.addVars(Model, vtype=GRB.BINARY, name=\"toxic\")\n",
    "v2 = m.addVars(Model, vtype=GRB.BINARY, name=\"obscene\")\n",
    "\n",
    "g = m.addVars(Model, name=\"gap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('OvR_identity_hate', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('BinaryRelevance', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('MLkNN', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('ClassifierChain', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('LabelPowerset', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_identity_hate', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_identity_hate', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_identity_hate', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_identity_hate', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_identity_hate', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_toxic', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_severe_toxic', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'threat'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_obscene', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_threat', 'insult'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'identity_hate'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'severe_toxic'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'obscene'): <gurobi.Var *Awaiting Model Update*>,\n",
       " ('OvR_insult', 'threat'): <gurobi.Var *Awaiting Model Update*>}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task constraints\n",
    "tasks = m.addConstrs((v.sum('*',t) <= 1 for t in T), name='task')\n",
    "t1 = m.addConstr(v1.sum() == 1, name='t1')\n",
    "t2 = m.addConstr(v2.sum() == 1, name='t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy constraint\n",
    "budget = m.addConstr((v1.prod(acc_task1)*v2.prod(acc_task2) >= Accuracy), name='budget')\n",
    "# v1.prod(acc_task1) + v2.prod(acc_task2) - v1.prod(acc_task1) * v2.prod(acc_task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective\n",
    "m.setObjective(v1.prod(cost_task1) + v2.prod(cost_task2),GRB.MINIMIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 9.0.3 build v9.0.3rc0 (mac64)\n",
      "Optimize a model with 8 rows, 150 columns and 80 nonzeros\n",
      "Model fingerprint: 0xeeb6f1ab\n",
      "Model has 1 quadratic constraint\n",
      "Variable types: 10 continuous, 140 integer (140 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  QMatrix range    [8e-01, 9e-01]\n",
      "  Objective range  [4e-04, 2e+02]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 1e+00]\n",
      "  QRHS range       [8e-01, 8e-01]\n",
      "Presolve removed 6 rows and 138 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 2 rows, 12 columns, 12 nonzeros\n",
      "Presolved model has 1 quadratic constraint(s)\n",
      "Variable types: 0 continuous, 12 integer (12 binary)\n",
      "\n",
      "Root relaxation: objective 7.220000e-04, 2 iterations, 0.00 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0       0.0007220    0.00072  0.00%     -    0s\n",
      "\n",
      "Explored 0 nodes (2 simplex iterations) in 0.02 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 0.000722 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 7.220000000000e-04, best bound 7.220000000000e-04, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "m.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obscene[OvR_obscene] 1.0 0.95 0.00036\n",
      "toxic[OvR_toxic] 1.0 0.898 0.00037\n",
      "Total matching score:  0.00073\n"
     ]
    }
   ],
   "source": [
    "# Compute total matching score from assignment variables\n",
    "total_matching_score = 0\n",
    "for model in M:\n",
    "    if v1[model].x > 1e-6:\n",
    "        print(v1[model].varName, v1[model].x, acc_task1[model], cost_task1[model]) \n",
    "        total_matching_score += cost_task1[model]*v1[model].x\n",
    "    if v2[model].x > 1e-6:\n",
    "        print(v2[model].varName, v2[model].x, acc_task2[model], cost_task2[model]) \n",
    "        total_matching_score += cost_task2[model]*v2[model].x\n",
    "\n",
    "print('Total matching score: ', total_matching_score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
